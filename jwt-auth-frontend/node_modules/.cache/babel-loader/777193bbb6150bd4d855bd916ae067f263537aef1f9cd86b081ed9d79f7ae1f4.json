{"ast":null,"code":"\"use strict\";\n\n// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\nvar __createBinding = this && this.__createBinding || (Object.create ? function (o, m, k, k2) {\n  if (k2 === undefined) k2 = k;\n  var desc = Object.getOwnPropertyDescriptor(m, k);\n  if (!desc || (\"get\" in desc ? !m.__esModule : desc.writable || desc.configurable)) {\n    desc = {\n      enumerable: true,\n      get: function () {\n        return m[k];\n      }\n    };\n  }\n  Object.defineProperty(o, k2, desc);\n} : function (o, m, k, k2) {\n  if (k2 === undefined) k2 = k;\n  o[k2] = m[k];\n});\nvar __setModuleDefault = this && this.__setModuleDefault || (Object.create ? function (o, v) {\n  Object.defineProperty(o, \"default\", {\n    enumerable: true,\n    value: v\n  });\n} : function (o, v) {\n  o[\"default\"] = v;\n});\nvar __importStar = this && this.__importStar || function (mod) {\n  if (mod && mod.__esModule) return mod;\n  var result = {};\n  if (mod != null) for (var k in mod) if (k !== \"default\" && Object.prototype.hasOwnProperty.call(mod, k)) __createBinding(result, mod, k);\n  __setModuleDefault(result, mod);\n  return result;\n};\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.Transcriptions = void 0;\nconst resource_1 = require(\"../../resource.js\");\nconst Core = __importStar(require(\"../../core.js\"));\nclass Transcriptions extends resource_1.APIResource {\n  create(body, options) {\n    return this._client.post('/audio/transcriptions', Core.multipartFormRequestOptions({\n      body,\n      ...options\n    }));\n  }\n}\nexports.Transcriptions = Transcriptions;","map":{"version":3,"names":["resource_1","require","Core","__importStar","Transcriptions","APIResource","create","body","options","_client","post","multipartFormRequestOptions","exports"],"sources":["/Users/harshitrishav/node_modules/openai/src/resources/audio/transcriptions.ts"],"sourcesContent":["// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../../resource';\nimport * as Core from '../../core';\nimport * as AudioAPI from './audio';\n\nexport class Transcriptions extends APIResource {\n  /**\n   * Transcribes audio into the input language.\n   */\n  create(\n    body: TranscriptionCreateParams<'json' | undefined>,\n    options?: Core.RequestOptions,\n  ): Core.APIPromise<Transcription>;\n  create(\n    body: TranscriptionCreateParams<'verbose_json'>,\n    options?: Core.RequestOptions,\n  ): Core.APIPromise<TranscriptionVerbose>;\n  create(\n    body: TranscriptionCreateParams<'srt' | 'vtt' | 'text'>,\n    options?: Core.RequestOptions,\n  ): Core.APIPromise<string>;\n  create(body: TranscriptionCreateParams, options?: Core.RequestOptions): Core.APIPromise<Transcription>;\n  create(\n    body: TranscriptionCreateParams,\n    options?: Core.RequestOptions,\n  ): Core.APIPromise<TranscriptionCreateResponse | string> {\n    return this._client.post('/audio/transcriptions', Core.multipartFormRequestOptions({ body, ...options }));\n  }\n}\n\n/**\n * Represents a transcription response returned by model, based on the provided\n * input.\n */\nexport interface Transcription {\n  /**\n   * The transcribed text.\n   */\n  text: string;\n}\n\nexport interface TranscriptionSegment {\n  /**\n   * Unique identifier of the segment.\n   */\n  id: number;\n\n  /**\n   * Average logprob of the segment. If the value is lower than -1, consider the\n   * logprobs failed.\n   */\n  avg_logprob: number;\n\n  /**\n   * Compression ratio of the segment. If the value is greater than 2.4, consider the\n   * compression failed.\n   */\n  compression_ratio: number;\n\n  /**\n   * End time of the segment in seconds.\n   */\n  end: number;\n\n  /**\n   * Probability of no speech in the segment. If the value is higher than 1.0 and the\n   * `avg_logprob` is below -1, consider this segment silent.\n   */\n  no_speech_prob: number;\n\n  /**\n   * Seek offset of the segment.\n   */\n  seek: number;\n\n  /**\n   * Start time of the segment in seconds.\n   */\n  start: number;\n\n  /**\n   * Temperature parameter used for generating the segment.\n   */\n  temperature: number;\n\n  /**\n   * Text content of the segment.\n   */\n  text: string;\n\n  /**\n   * Array of token IDs for the text content.\n   */\n  tokens: Array<number>;\n}\n\n/**\n * Represents a verbose json transcription response returned by model, based on the\n * provided input.\n */\nexport interface TranscriptionVerbose {\n  /**\n   * The duration of the input audio.\n   */\n  duration: string;\n\n  /**\n   * The language of the input audio.\n   */\n  language: string;\n\n  /**\n   * The transcribed text.\n   */\n  text: string;\n\n  /**\n   * Segments of the transcribed text and their corresponding details.\n   */\n  segments?: Array<TranscriptionSegment>;\n\n  /**\n   * Extracted words and their corresponding timestamps.\n   */\n  words?: Array<TranscriptionWord>;\n}\n\nexport interface TranscriptionWord {\n  /**\n   * End time of the word in seconds.\n   */\n  end: number;\n\n  /**\n   * Start time of the word in seconds.\n   */\n  start: number;\n\n  /**\n   * The text content of the word.\n   */\n  word: string;\n}\n\n/**\n * Represents a transcription response returned by model, based on the provided\n * input.\n */\nexport type TranscriptionCreateResponse = Transcription | TranscriptionVerbose;\n\nexport interface TranscriptionCreateParams<\n  ResponseFormat extends AudioAPI.AudioResponseFormat | undefined = AudioAPI.AudioResponseFormat | undefined,\n> {\n  /**\n   * The audio file object (not file name) to transcribe, in one of these formats:\n   * flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.\n   */\n  file: Core.Uploadable;\n\n  /**\n   * ID of the model to use. Only `whisper-1` (which is powered by our open source\n   * Whisper V2 model) is currently available.\n   */\n  model: (string & {}) | AudioAPI.AudioModel;\n\n  /**\n   * The language of the input audio. Supplying the input language in\n   * [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) (e.g. `en`)\n   * format will improve accuracy and latency.\n   */\n  language?: string;\n\n  /**\n   * An optional text to guide the model's style or continue a previous audio\n   * segment. The\n   * [prompt](https://platform.openai.com/docs/guides/speech-to-text#prompting)\n   * should match the audio language.\n   */\n  prompt?: string;\n\n  /**\n   * The format of the output, in one of these options: `json`, `text`, `srt`,\n   * `verbose_json`, or `vtt`.\n   */\n  response_format?: ResponseFormat;\n\n  /**\n   * The sampling temperature, between 0 and 1. Higher values like 0.8 will make the\n   * output more random, while lower values like 0.2 will make it more focused and\n   * deterministic. If set to 0, the model will use\n   * [log probability](https://en.wikipedia.org/wiki/Log_probability) to\n   * automatically increase the temperature until certain thresholds are hit.\n   */\n  temperature?: number;\n\n  /**\n   * The timestamp granularities to populate for this transcription.\n   * `response_format` must be set `verbose_json` to use timestamp granularities.\n   * Either or both of these options are supported: `word`, or `segment`. Note: There\n   * is no additional latency for segment timestamps, but generating word timestamps\n   * incurs additional latency.\n   */\n  timestamp_granularities?: Array<'word' | 'segment'>;\n}\n\nexport declare namespace Transcriptions {\n  export {\n    type Transcription as Transcription,\n    type TranscriptionSegment as TranscriptionSegment,\n    type TranscriptionVerbose as TranscriptionVerbose,\n    type TranscriptionWord as TranscriptionWord,\n    type TranscriptionCreateResponse as TranscriptionCreateResponse,\n    type TranscriptionCreateParams as TranscriptionCreateParams,\n  };\n}\n"],"mappings":";;AAAA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAEA,MAAAA,UAAA,GAAAC,OAAA;AACA,MAAAC,IAAA,GAAAC,YAAA,CAAAF,OAAA;AAGA,MAAaG,cAAe,SAAQJ,UAAA,CAAAK,WAAW;EAiB7CC,MAAMA,CACJC,IAA+B,EAC/BC,OAA6B;IAE7B,OAAO,IAAI,CAACC,OAAO,CAACC,IAAI,CAAC,uBAAuB,EAAER,IAAI,CAACS,2BAA2B,CAAC;MAAEJ,IAAI;MAAE,GAAGC;IAAO,CAAE,CAAC,CAAC;EAC3G;;AAtBFI,OAAA,CAAAR,cAAA,GAAAA,cAAA","ignoreList":[]},"metadata":{},"sourceType":"script","externalDependencies":[]}